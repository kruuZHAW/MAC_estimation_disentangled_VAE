{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_traffic_generation.tcvae import TCVAE\n",
    "from deep_traffic_generation.core.datasets import TrafficDataset\n",
    "from traffic.core import Traffic\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = TrafficDataset.from_file(\n",
    "    (\"../deep_traffic_generation/data/training_datasets/takeoffs_south_LFPO_07.pkl\"),\n",
    "    features=[\"track\", \"groundspeed\", \"altitude\", \"timedelta\"],\n",
    "    scaler=MinMaxScaler(feature_range=(-1,1)),\n",
    "    # scaler=None,\n",
    "    shape=\"image\",\n",
    "    info_params={\"features\": [\"latitude\", \"longitude\"], \"index\": -1},\n",
    ")\n",
    "dataset1_bis = TrafficDataset.from_file(\n",
    "    (\"../deep_traffic_generation/data/training_datasets/takeoffs_south_LFPO_07.pkl\"),\n",
    "    features=[\"track\", \"groundspeed\", \"altitude\", \"timedelta\"],\n",
    "    # scaler=MinMaxScaler(feature_range=(-1,1)),\n",
    "    scaler=None,\n",
    "    shape=\"linear\",\n",
    "    info_params={\"features\": [\"latitude\", \"longitude\"], \"index\": -1},\n",
    ")\n",
    "\n",
    "dataset2 = TrafficDataset.from_file(\n",
    "    (\"../deep_traffic_generation/data/training_datasets/landings_south_LFPO_06.pkl\"),\n",
    "    features=[\"track\", \"groundspeed\", \"altitude\", \"timedelta\"],\n",
    "    scaler=MinMaxScaler(feature_range=(-1,1)),\n",
    "    # scaler=None,\n",
    "    shape=\"image\",\n",
    "    info_params={\"features\": [\"latitude\", \"longitude\"], \"index\": -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic1 = Traffic.from_file(\"../deep_traffic_generation/data/training_datasets/takeoffs_south_LFPO_07.pkl\")\n",
    "traffic2 = Traffic.from_file(\"../deep_traffic_generation/data/training_datasets/landings_south_LFPO_06.pkl\")\n",
    "\n",
    "data1 = np.stack(\n",
    "    list([f.flight_id, f.start, f.stop] for f in traffic1)\n",
    ")\n",
    "data2 = np.stack(\n",
    "    list([f.flight_id, f.start, f.stop] for f in traffic2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairs(iter):\n",
    "    x, y = iter\n",
    "    delta_t = y[1] - x[1]\n",
    "    len_x = x[2] - x[1]\n",
    "    len_y = y[2] - y[1]\n",
    "\n",
    "    #modified according to sandobox_tcas\n",
    "    if delta_t < -len_y or (delta_t > len_x):\n",
    "        return\n",
    "    \n",
    "    #make sure that delta_t is smaller than the total duration of the reference (the takeoff)\n",
    "    elif (delta_t < len_x): \n",
    "        return np.array([x[0], y[0], delta_t.total_seconds()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "with Pool(processes=os.cpu_count()) as p: \n",
    "    pairs = p.map(calculate_pairs, itertools.product(data1,data2))\n",
    "    p.close()\n",
    "    p.join()\n",
    "pairs = np.stack([x for x in pairs if x is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "for i in range(len(pairs)):\n",
    "    a.append(dataset1.get_flight(pairs[i,0]))\n",
    "    b.append(dataset2.get_flight(pairs[i,1]))\n",
    "    \n",
    "a = torch.stack(a)\n",
    "b = torch.stack(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TCVAE:\n\tMissing key(s) in state_dict: \"lsr.z_loc.1.weight\", \"lsr.z_loc.1.bias\", \"lsr.z_loc.1.running_mean\", \"lsr.z_loc.1.running_var\", \"lsr.z_loc.3.weight\", \"lsr.z_loc.3.bias\", \"lsr.z_loc.4.weight\", \"lsr.z_loc.4.bias\", \"lsr.z_loc.4.running_mean\", \"lsr.z_loc.4.running_var\", \"lsr.pseudo_inputs_NN.1.weight\", \"lsr.pseudo_inputs_NN.1.bias\", \"lsr.pseudo_inputs_NN.1.running_mean\", \"lsr.pseudo_inputs_NN.1.running_var\", \"lsr.pseudo_inputs_NN.3.weight\", \"lsr.pseudo_inputs_NN.3.bias\". \n\tUnexpected key(s) in state_dict: \"lsr.pseudo_inputs_NN.2.weight\", \"lsr.pseudo_inputs_NN.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m filenames1 \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(walk(\u001b[39m\"\u001b[39m\u001b[39m../deep_traffic_generation/lightning_logs/tcvae/version_0/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/\u001b[39m\u001b[39m\"\u001b[39m), (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, []))[\u001b[39m2\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m VAE1 \u001b[39m=\u001b[39m TCVAE\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m../deep_traffic_generation/lightning_logs/tcvae/version_0/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcheckpoints/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m filenames1[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     dataset_params\u001b[39m=\u001b[39;49mdataset1\u001b[39m.\u001b[39;49mparameters,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m VAE1\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsrv-lab-t-250.zhaw.ch/cluster/home/kruu/TCAS_generation/notebooks/sandbox_metaDataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m filenames2 \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(walk(\u001b[39m\"\u001b[39m\u001b[39m../deep_traffic_generation/lightning_logs/tcvae/version_1/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/\u001b[39m\u001b[39m\"\u001b[39m), (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, []))[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tcas/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:137\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[1;32m     66\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    138\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    139\u001b[0m         checkpoint_path,\n\u001b[1;32m    140\u001b[0m         map_location,\n\u001b[1;32m    141\u001b[0m         hparams_file,\n\u001b[1;32m    142\u001b[0m         strict,\n\u001b[1;32m    143\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tcas/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:205\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39m, checkpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39;49m, checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tcas/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:259\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m    258\u001b[0m \u001b[39m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m keys \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m], strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m strict:\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m keys\u001b[39m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/miniconda3/envs/tcas/lib/python3.9/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TCVAE:\n\tMissing key(s) in state_dict: \"lsr.z_loc.1.weight\", \"lsr.z_loc.1.bias\", \"lsr.z_loc.1.running_mean\", \"lsr.z_loc.1.running_var\", \"lsr.z_loc.3.weight\", \"lsr.z_loc.3.bias\", \"lsr.z_loc.4.weight\", \"lsr.z_loc.4.bias\", \"lsr.z_loc.4.running_mean\", \"lsr.z_loc.4.running_var\", \"lsr.pseudo_inputs_NN.1.weight\", \"lsr.pseudo_inputs_NN.1.bias\", \"lsr.pseudo_inputs_NN.1.running_mean\", \"lsr.pseudo_inputs_NN.1.running_var\", \"lsr.pseudo_inputs_NN.3.weight\", \"lsr.pseudo_inputs_NN.3.bias\". \n\tUnexpected key(s) in state_dict: \"lsr.pseudo_inputs_NN.2.weight\", \"lsr.pseudo_inputs_NN.2.bias\". "
     ]
    }
   ],
   "source": [
    "filenames1 = next(walk(\"../deep_traffic_generation/lightning_logs/tcvae/version_0/\"+ \"checkpoints/\"), (None, None, []))[2]\n",
    "VAE1 = TCVAE.load_from_checkpoint(\n",
    "    \"../deep_traffic_generation/lightning_logs/tcvae/version_0/\" + \"checkpoints/\" + filenames1[0],\n",
    "    dataset_params=dataset1.parameters,\n",
    ")\n",
    "VAE1.eval()\n",
    "\n",
    "filenames2 = next(walk(\"../deep_traffic_generation/lightning_logs/tcvae/version_1/\"+ \"checkpoints/\"), (None, None, []))[2]\n",
    "VAE2 = TCVAE.load_from_checkpoint(\n",
    "    \"../deep_traffic_generation/lightning_logs/tcvae/version_1/\" + \"checkpoints/\" + filename2[0],\n",
    "    dataset_params=dataset2.parameters,\n",
    ")\n",
    "VAE2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We selected pairs of trajectories from traffic1 and traffic2\n",
    "\n",
    "- We make one dataset of takoffs for VAE1 and one dataset of landings for VAe2 based on the pairs (in one dataset, one trajectory may appear several times). Each input tensor is sorted according the pairs we identified.\n",
    "\n",
    "- We calculate the corresponding latent representations in VAE1 and VAE2 (we may take the means of each posterior or one randomly point). The output tensors are sorted according the pairs we identified.\n",
    "\n",
    "- We concatenate the two tensors and train the meta VAE. input: linear tensor of size(nb_pairs, 2, latent_dim) or size(nb_pairs, 2*latent_dim). Either we only do TCN VAE or Dense VAE.  \n",
    "\n",
    "- If everything works well: We are able to generate a new pair of latent representations with the meta VAE. Then we pass each representation in the corresponding VAE to get the reconstructed trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tcas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a6671a896d78c2485d323f2ffc62bdd1325d269c4236792e18439313d8ecb79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
